{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iagunov/spark_data_analysis/blob/main/iagunov_spark_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qYX1n8RrCc-F"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "4Fm3IrsLDBcB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "kAowd2J-DSyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "dfdhcirkGDcj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "KrmGjRKuok3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "G0-AdX-zFqSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $SPARK_HOME"
      ],
      "metadata": {
        "id": "HauZPaywJs98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark import  SparkContext\n",
        "from pyspark.sql import functions as f\n",
        "appName = \"PySpark Task 1\"\n",
        "#master = \"spark://10.3.100.4:7077\"\n",
        "master = 'local[*]'\n",
        "# Create Spark session with Hive supported.\n",
        "spark = SparkSession.builder \\\n",
        "    .enableHiveSupport() \\\n",
        "    .appName(appName) \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "SjDRZ0vTHbQT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emp = spark.read.json(\"/content/sample_data/anscombe.json\")\n",
        "df_emp.show(5,False)"
      ],
      "metadata": {
        "id": "NpoFPaObPv-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 1**\n",
        "\n",
        "Напишите запрос, возвращающий строку следующего вида для каждого сотрудника: \"<фамилия> зарабатывает <зарплата> каждый месяц, но хочет получать <зарплата * 3>\".\n",
        "Назовите колонку 'Dream Salaries'. Результат сохранить в формате parquet со сжатием GZIP"
      ],
      "metadata": {
        "id": "AY3upU4pq9CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из CSV-файла в DataFrame\n",
        "df_emp = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \"\\t\")\n",
        "    .load(\"/content/employees\")\n",
        ")\n",
        "\n",
        "# Вывод схемы DataFrame\n",
        "df_emp.printSchema()\n",
        "\n",
        "# Вывод первых 5 строк DataFrame\n",
        "df_emp.show(5)"
      ],
      "metadata": {
        "id": "Sy50MFscrUa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание временного представления для DataFrame\n",
        "df_emp.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# SQL-запрос для формирования нового DataFrame с использованием Spark SQL\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        CONCAT(\n",
        "            CAST(employees.last_name AS VARCHAR(50)),\n",
        "            ' зарабатывает ',\n",
        "            CAST(employees.salary AS VARCHAR(50)),\n",
        "            ' каждый месяц, но хочет получать ',\n",
        "            CAST(employees.salary * 3 AS VARCHAR(50))\n",
        "        ) AS `Dream Salaries`\n",
        "    FROM employees\n",
        "\"\"\")\n",
        "\n",
        "# Вывод первых 3 строк DataFrame с учетом длинных строк\n",
        "df.show(3, truncate=False)"
      ],
      "metadata": {
        "id": "m7kups4RwCGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запись DataFrame в формате Parquet с указанием сжатия GZIP и режима \"overwrite\"\n",
        "df.write.format(\"parquet\") \\\n",
        "        .option(\"compression\", \"gzip\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"data_out/task_1\")\n",
        "\n",
        "# Чтение данных из Parquet-файла с указанием сжатия\n",
        "spark.read.format(\"parquet\") \\\n",
        "          .option(\"compression\", \"gzip\") \\\n",
        "          .load(\"data_out/task_1\") \\\n",
        "          .show(truncate=False)"
      ],
      "metadata": {
        "id": "QY4svzEe04mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**\n",
        "\n",
        "Напишите запрос, возвращающий адреса всех департаментов. Запрос должен возвращать ID локации, адрес (улицу), город, штат и страну."
      ],
      "metadata": {
        "id": "fQPZysfDrWdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из файлов в DataFrame\n",
        "df_dep = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \",\")\n",
        "    .load(\"/content/departments\")\n",
        ")\n",
        "df_loc = (\n",
        "    spark.read.format(\"parquet\")\n",
        "    .option(\"compression\", \"gzip\")\n",
        "    .load(\"/content/part-00000-f3626ac3-ae1a-49d5-8aa5-760f2e41b579-c000.gz.parquet\")\n",
        ")\n",
        "df_cont = (\n",
        "    spark.read.format(\"orc\")\n",
        "    .option(\"codec\", \"snappy\")\n",
        "    .load(\"/content/part-00000-ebe0451d-1509-4b7e-a7fd-abeb9e5488ef-c000.snappy.orc\")\n",
        ")\n",
        "\n",
        "# Вывод схемы DataFrame\n",
        "df_dep.printSchema()\n",
        "df_loc.printSchema()\n",
        "df_cont.printSchema()\n",
        "\n",
        "# Вывод первых 5 строк DataFrame\n",
        "df_dep.show(5, truncate=False)\n",
        "df_loc.show(5, truncate=False)\n",
        "df_cont.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "V9X0_93crc6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание временного представления для DataFrame\n",
        "df_dep.createOrReplaceTempView(\"departments\")\n",
        "df_loc.createOrReplaceTempView(\"locations\")\n",
        "df_cont.createOrReplaceTempView(\"countries\")\n",
        "\n",
        "# SQL-запрос для формирования нового DataFrame с использованием Spark SQL\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        location_id,\n",
        "        street_address,\n",
        "        city,\n",
        "        state_province,\n",
        "        country_name\n",
        "    FROM departments\n",
        "    NATURAL JOIN locations\n",
        "    NATURAL JOIN countries;\n",
        "\"\"\")\n",
        "\n",
        "# Вывод первых 3 строк DataFrame с учетом длинных строк\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "U0n-zNu4rCIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение результата\n",
        "df.write.format(\"orc\") \\\n",
        "        .option(\"codec\", \"snappy\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"data_out/task_2\")\n",
        "\n",
        "# Проверка результата\n",
        "spark.read.format(\"orc\") \\\n",
        "          .option(\"codec\", \"snappy\") \\\n",
        "          .load(\"data_out/task_2\") \\\n",
        "          .show(truncate=False)"
      ],
      "metadata": {
        "id": "k2qEFoBc6GpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 3**\n",
        "\n",
        "Напишите запрос, возвращающий фамилию, ID отдела и наименование отдела для каждого сотрудника; Результат сохранить в формате avro со сжатием GZIP"
      ],
      "metadata": {
        "id": "sR9u64_2rfNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из CSV-файлов в DataFrame\n",
        "df_emp = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \"\\t\")\n",
        "    .load(\"/content/employees\")\n",
        ")\n",
        "df_dep = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \",\")\n",
        "    .load(\"/content/departments\")\n",
        ")\n",
        "\n",
        "# Вывод схемы DataFrame\n",
        "df_emp.printSchema()\n",
        "df_dep.printSchema()\n",
        "\n",
        "# Вывод первых 5 строк DataFrame\n",
        "df_emp.show(5)\n",
        "df_dep.show(5)"
      ],
      "metadata": {
        "id": "fHBeet_6rlOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание временного представления для DataFrame\n",
        "df_emp.createOrReplaceTempView(\"employees\")\n",
        "df_dep.createOrReplaceTempView(\"departments\")\n",
        "\n",
        "\n",
        "# SQL-запрос для формирования нового DataFrame с использованием Spark SQL\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "\t    emp.last_name,\n",
        "\t    emp.department_id,\n",
        "\t    dp.department_name\n",
        "    FROM\n",
        "\t    employees emp\n",
        "    INNER JOIN\n",
        "\t    departments dp ON emp.department_id = dp.department_id;\n",
        "\"\"\")\n",
        "\n",
        "# Вывод первых 3 строк DataFrame с учетом длинных строк\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "ntRnLaSu-ofj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение результата в формате avro со сжатием gzip\n",
        "df.write.format(\"avro\") \\\n",
        "        .option(\"codec\", \"gzip\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"data_out/task_3.avro\")\n",
        "\n",
        "# Проверка результата\n",
        "spark.read.format(\"avro\") \\\n",
        "          .option(\"codec\", \"gzip\") \\\n",
        "          .load(\"data_out/task_3.avro\") \\\n",
        "          .show(truncate=False)"
      ],
      "metadata": {
        "id": "BFa_2zv7_dy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 4**\n",
        "\n",
        "Напишите запрос, возвращающий фамилию, ID сотрудника, фамилию менеджера и ID менеджера для каждого сотрудника (для сотрудников, у которых нет менеджера, в этих колонках должен быть NULL).\n",
        "Назовите колонки 'Employee', 'Emp#', 'Manager', 'Mgr#'. Результат сохранить в формате avro со сжатием Snappy"
      ],
      "metadata": {
        "id": "M9CQslKVrp4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из CSV-файлов в DataFrame\n",
        "df_emp = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \"\\t\")\n",
        "    .load(\"/content/employees\")\n",
        ")\n",
        "\n",
        "# Вывод схемы DataFrame\n",
        "df_emp.printSchema()\n",
        "\n",
        "# Вывод первых 5 строк DataFrame\n",
        "df_emp.show(5)"
      ],
      "metadata": {
        "id": "Igt3AT6CrunH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание временного представления для DataFrame\n",
        "df_emp.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "\n",
        "# SQL-запрос для формирования нового DataFrame с использованием Spark SQL\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        e.last_name AS Employee,\n",
        "        e.employee_id AS `Emp`,\n",
        "        m.last_name AS Manager,\n",
        "        m.employee_id AS `Mgr`\n",
        "    FROM\n",
        "        employees e\n",
        "    LEFT JOIN\n",
        "        employees m ON e.manager_id = m.employee_id;\n",
        "\"\"\")\n",
        "\n",
        "# Вывод первых 3 строк DataFrame с учетом длинных строк\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "919GAfTCb862"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Использование символа `#` в названиях столбцов не поддерживается snappy, использования обратных ковычек и обратного слеша не помогает.\n",
        "# Можно либо переименовать столбцы либо использовать другой кодек который это поддерживает, например deflate, я выбрал переименовать столбцы.\n",
        "\n",
        "# Сохранение результата\n",
        "df.write.format(\"avro\") \\\n",
        "        .option(\"codec\", \"snappy\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"data_out/task_4.avro\")\n",
        "\n",
        "# Проверка результата\n",
        "spark.read.format(\"avro\") \\\n",
        "          .option(\"codec\", \"snappy\") \\\n",
        "          .load(\"data_out/task_4.avro\") \\\n",
        "          .show(truncate=False)"
      ],
      "metadata": {
        "id": "_-FFc3QRcgmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 5**\n",
        "\n",
        "Напишите запрос, возвращающий фамилии и зарплаты всех сотрудников, которые подчиняются сотруднику King.\n",
        "Используйте подзапрос."
      ],
      "metadata": {
        "id": "ULhnnI5Qrwmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из CSV-файла в DataFrame\n",
        "df_emp = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \"\\t\")\n",
        "    .load(\"/content/employees\")\n",
        ")\n",
        "\n",
        "# Вывод схемы DataFrame\n",
        "df_emp.printSchema()\n",
        "\n",
        "# Вывод первых 5 строк DataFrame\n",
        "df_emp.show(5)"
      ],
      "metadata": {
        "id": "ArIQu5bnr1zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание временного представления \"employees\" для DataFrame\n",
        "df_emp.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# SQL-запрос для формирования нового DataFrame с использованием Spark SQL\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "\t    last_name,\n",
        "\t    salary\n",
        "    FROM employees\n",
        "    WHERE manager_id IN (SELECT employee_id FROM employees WHERE last_name = 'King')\n",
        "\"\"\")\n",
        "\n",
        "# Вывод строк DataFrame без обрезки значений\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "wWcMKOgO-Z0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запись DataFrame в формате Parquet с указанием сжатия GZIP и режима \"overwrite\"\n",
        "df.write.format(\"parquet\") \\\n",
        "        .option(\"compression\", \"gzip\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"data_out/task_5\")\n",
        "\n",
        "# Чтение данных из Parquet-файла с указанием сжатия\n",
        "spark.read.format(\"parquet\") \\\n",
        "          .option(\"compression\", \"gzip\") \\\n",
        "          .load(\"data_out/task_5\") \\\n",
        "          .show(truncate=False)"
      ],
      "metadata": {
        "id": "rLX33CD1CbEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 6**\n",
        "\n",
        "Напишите запрос, возвращающий фамилии всех сотрудников, получающих больше, чем любой сотрудник отдела с ID 60."
      ],
      "metadata": {
        "id": "0osV1GQqr5kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из CSV-файла в DataFrame\n",
        "df_emp = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \"\\t\")\n",
        "    .load(\"/content/employees\")\n",
        ")\n",
        "\n",
        "# Вывод схемы DataFrame\n",
        "df_emp.printSchema()\n",
        "\n",
        "# Вывод первых 5 строк DataFrame\n",
        "df_emp.show(5)"
      ],
      "metadata": {
        "id": "Lir5oP1qr-c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание временного представления \"employees\" для DataFrame\n",
        "df_emp.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# SQL-запрос для формирования нового DataFrame с использованием Spark SQL\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "\t    last_name\n",
        "    FROM employees\n",
        "    WHERE salary > (SELECT MAX(salary) FROM employees WHERE department_id = 60)\n",
        "\"\"\")\n",
        "\n",
        "# Вывод строк DataFrame без обрезки значений\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "b8VU-zl5pyIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запись DataFrame в формате Parquet с указанием сжатия GZIP и режима \"overwrite\"\n",
        "df.write.format(\"parquet\") \\\n",
        "        .option(\"compression\", \"gzip\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"data_out/task_6\")\n",
        "\n",
        "# Чтение данных из Parquet-файла с указанием сжатия\n",
        "spark.read.format(\"parquet\") \\\n",
        "          .option(\"compression\", \"gzip\") \\\n",
        "          .load(\"data_out/task_6\") \\\n",
        "          .show(truncate=False)"
      ],
      "metadata": {
        "id": "xswmgs6yqZ53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 7**\n",
        "\n",
        "Напишите запрос, возвращающий ID, фамилии и зарплаты всех сотрудников, работающих в одном отделе с работником, в чьей фамилии есть буква 'u' и получающих больше средней зарплаты в компании."
      ],
      "metadata": {
        "id": "gZISlX1_sBQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из CSV-файла в DataFrame\n",
        "df_emp = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \"\\t\")\n",
        "    .load(\"/content/employees\")\n",
        ")\n",
        "\n",
        "# Вывод схемы DataFrame\n",
        "df_emp.printSchema()\n",
        "\n",
        "# Вывод первых 5 строк DataFrame\n",
        "df_emp.show(5)"
      ],
      "metadata": {
        "id": "mhJJN0rvsFSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание временного представления \"employees\" для DataFrame\n",
        "df_emp.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# SQL-запрос для формирования нового DataFrame с использованием Spark SQL\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "\t    employee_id,\n",
        "\t    last_name,\n",
        "\t    salary\n",
        "    FROM employees\n",
        "    WHERE department_id = (SELECT department_id FROM employees WHERE upper(last_name) LIKE '%U%' LIMIT 1) AND\n",
        "\t        salary > (SELECT AVG(salary) FROM employees)\n",
        "\"\"\")\n",
        "\n",
        "# Вывод строк DataFrame без обрезки значений\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "LA1f7-eMGJd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение результата\n",
        "df.write.format(\"orc\") \\\n",
        "        .option(\"codec\", \"snappy\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"data_out/task_7\")\n",
        "\n",
        "# Проверка результата\n",
        "spark.read.format(\"orc\") \\\n",
        "          .option(\"codec\", \"snappy\") \\\n",
        "          .load(\"data_out/task_7\") \\\n",
        "          .show(truncate=False)"
      ],
      "metadata": {
        "id": "3j_rFO11JJZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 8**\n",
        "\n",
        "Выведите фамилии, id отдела и название отдела для всех сотрудников, не привязанных ни к одному отделу, а также список отделов, к которым не привязан ни один сотрудник."
      ],
      "metadata": {
        "id": "SRIUoN6hsH8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из CSV-файла в DataFrame\n",
        "df_emp = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \"\\t\")\n",
        "    .load(\"/content/employees\")\n",
        ")\n",
        "df_dep = (\n",
        "    spark.read.format(\"csv\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"delimiter\", \",\")\n",
        "    .load(\"/content/departments\")\n",
        ")\n",
        "\n",
        "# Вывод схемы DataFrame\n",
        "df_emp.printSchema()\n",
        "df_dep.printSchema()\n",
        "\n",
        "# Вывод первых 5 строк DataFrame\n",
        "df_emp.show(5)\n",
        "df_dep.show(5)"
      ],
      "metadata": {
        "id": "aV-28uBEsK42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание временного представления для DataFrames\n",
        "df_emp.createOrReplaceTempView(\"employees\")\n",
        "df_dep.createOrReplaceTempView(\"departments\")\n",
        "\n",
        "# SQL-запрос для формирования нового DataFrame с использованием Spark SQL\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "\t    emp.employee_id,\n",
        "\t    emp.last_name\n",
        "    FROM employees emp\n",
        "    WHERE emp.department_id IS NULL\n",
        "\n",
        "    UNION ALL\n",
        "\n",
        "    SELECT\n",
        "\t    dep.department_id,\n",
        "\t    dep.department_name\n",
        "    FROM employees emp\n",
        "    RIGHT JOIN departments dep USING(department_id)\n",
        "    WHERE emp.department_id IS NULL;\n",
        "\"\"\")\n",
        "\n",
        "# Вывод строк DataFrame без обрезки значений\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "w5VTAg9FR5Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение результата\n",
        "df.write.format(\"orc\") \\\n",
        "        .option(\"codec\", \"snappy\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(\"data_out/task_8\")\n",
        "\n",
        "# Проверка результата\n",
        "spark.read.format(\"orc\") \\\n",
        "          .option(\"codec\", \"snappy\") \\\n",
        "          .load(\"data_out/task_8\") \\\n",
        "          .show(truncate=False)"
      ],
      "metadata": {
        "id": "xzai39FTSawC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3ddA093kh3_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}